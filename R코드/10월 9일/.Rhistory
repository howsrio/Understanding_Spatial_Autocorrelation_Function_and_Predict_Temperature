subMat_12=total_CovMat[1:nrow(rem_point),(nrow(rem_point)+1):nrow(total_point)]
subMat_21=total_CovMat[(nrow(rem_point)+1):nrow(total_point),1:nrow(rem_point)]
subMat_22=total_CovMat[(nrow(rem_point)+1):nrow(total_point),(nrow(rem_point)+1):nrow(total_point)]
tryCatch({
inverse_subMat_11=solve(subMat_11)
sam_pred_vec=subMat_21%*%inverse_subMat_11%*%dev_matrix + mean
#After making kriging model, we have to check MSE to check 성능
MSE= mean((sam_pred_vec - test_data$tem)^2)
},error=function(e){
MSE =NA
})
#kriging(based on Lecture 03)
return(c(MLE[1], range, MLE[3], nugget, MSE))
}
for(file in folder[3:16]){
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
count=count+1
}
NLM
rem_point=as.matrix(train_data[,c("Long","Lat")])
sam_point=as.matrix(test_data[,c('Long',"Lat")])
train_dev=train_data$tem - mean(train_data$tem)
train.sp=SpatialPointsDataFrame(coords=as.matrix(train_data[,c('Long',"Lat")]),train_data,
proj4string=CRS("+proj=utm +zone=48 +datum=WGS84"))
chordal_matrix <<- theta_dist(rem_point)
#Find Initial value
v <- variogram((train_dev) ~ 1, locations = coordinates(rem_point), train.sp)
psill=max(v$gamma)
range= v$dist[length(v$dist)]
nugget <<- min(v$gamma)
smoothness_parameter= 1/2
parameters=c(psill, range, smoothness_parameter)
log_paras=log(abs(parameters)+0.001)
#Deviation matrix
mean <- mean(train_data$tem)
dev_matrix <<- as.matrix(train_data$tem - mean)
MLE=parameters
NLM=nlm(NegLogL_optim2,log_paras)
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
value
value= NA
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
is.an(value)
value= NA
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
if (is.na(value)){
value=0
}
value
#Calculate negative log likelihood for optim
NegLogL_optim2=function(log_paras){
value= NA
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
if (is.na(value)){
value=0
}
return(value)
}
rem_point=as.matrix(train_data[,c("Long","Lat")])
sam_point=as.matrix(test_data[,c('Long',"Lat")])
train_dev=train_data$tem - mean(train_data$tem)
train.sp=SpatialPointsDataFrame(coords=as.matrix(train_data[,c('Long',"Lat")]),train_data,
proj4string=CRS("+proj=utm +zone=48 +datum=WGS84"))
chordal_matrix <<- theta_dist(rem_point)
#Find Initial value
v <- variogram((train_dev) ~ 1, locations = coordinates(rem_point), train.sp)
psill=max(v$gamma)
range= v$dist[length(v$dist)]
nugget <<- min(v$gamma)
smoothness_parameter= 1/2
parameters=c(psill, range, smoothness_parameter)
log_paras=log(abs(parameters)+0.001)
#Deviation matrix
mean <- mean(train_data$tem)
dev_matrix <<- as.matrix(train_data$tem - mean)
MLE=parameters
NLM=nlm(NegLogL_optim2,log_paras)
NLM
MLE=exp(NLM$estimate)
MLE
parameters
cat('there is error in optim',exp(log_paras))
#Calculate negative log likelihood for optim
NegLogL_optim2=function(log_paras){
value= NA
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
if (is.na(value)){
cat('there is error in optim with',exp(log_paras))
value=0
}
return(value)
}
MLE_test2=function(train_data, test_data){
rem_point=as.matrix(train_data[,c("Long","Lat")])
sam_point=as.matrix(test_data[,c('Long',"Lat")])
train_dev=train_data$tem - mean(train_data$tem)
train.sp=SpatialPointsDataFrame(coords=as.matrix(train_data[,c('Long',"Lat")]),train_data,
proj4string=CRS("+proj=utm +zone=48 +datum=WGS84"))
chordal_matrix <<- theta_dist(rem_point)
#Find Initial value
v <- variogram((train_dev) ~ 1, locations = coordinates(rem_point), train.sp)
psill=max(v$gamma)
range= v$dist[length(v$dist)]
nugget <<- min(v$gamma)
smoothness_parameter= 1/2
parameters=c(psill, range, smoothness_parameter)
log_paras=log(abs(parameters)+0.001)
#Deviation matrix
mean <- mean(train_data$tem)
dev_matrix <<- as.matrix(train_data$tem - mean)
NLM=nlm(NegLogL_optim2,log_paras)
MLE=exp(NLM$estimate)
#Now, we are ready to predict temperature of unknown points
total_point=rbind(rem_point,sam_point)
#Make covariance matrix with expanded coordinates
#(확장된 좌표들의 covaraicne matrix를 만듭니다.)
total_chordal_dist = chordal_dist(total_point)
range=2*sin(MLE[2]/2)*6371
total_CovMat = covariance_func(total_chordal_dist, MLE[1], range, MLE[3], nugget)
#Divide covariance matrix into 4 submatrices
subMat_11=total_CovMat[1:nrow(rem_point),1:nrow(rem_point)]
subMat_12=total_CovMat[1:nrow(rem_point),(nrow(rem_point)+1):nrow(total_point)]
subMat_21=total_CovMat[(nrow(rem_point)+1):nrow(total_point),1:nrow(rem_point)]
subMat_22=total_CovMat[(nrow(rem_point)+1):nrow(total_point),(nrow(rem_point)+1):nrow(total_point)]
tryCatch({
inverse_subMat_11=solve(subMat_11)
sam_pred_vec=subMat_21%*%inverse_subMat_11%*%dev_matrix + mean
#After making kriging model, we have to check MSE to check 성능
MSE= mean((sam_pred_vec - test_data$tem)^2)
},error=function(e){
MSE =NA
})
#kriging(based on Lecture 03)
return(c(MLE[1], range, MLE[3], nugget, MSE))
}
for(file in folder[3:16]){
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
count=count+1
}
rem_point=as.matrix(train_data[,c("Long","Lat")])
sam_point=as.matrix(test_data[,c('Long',"Lat")])
train_dev=train_data$tem - mean(train_data$tem)
train.sp=SpatialPointsDataFrame(coords=as.matrix(train_data[,c('Long',"Lat")]),train_data,
proj4string=CRS("+proj=utm +zone=48 +datum=WGS84"))
chordal_matrix <<- theta_dist(rem_point)
#Find Initial value
v <- variogram((train_dev) ~ 1, locations = coordinates(rem_point), train.sp)
psill=max(v$gamma)
range= v$dist[length(v$dist)]
nugget <<- min(v$gamma)
smoothness_parameter= 1/2
parameters=c(psill, range, smoothness_parameter)
log_paras=log(abs(parameters)+0.001)
#Deviation matrix
mean <- mean(train_data$tem)
dev_matrix <<- as.matrix(train_data$tem - mean)
NLM=nlm(NegLogL_optim2,log_paras)
MLE=exp(NLM$estimate)
MLE
#Now, we are ready to predict temperature of unknown points
total_point=rbind(rem_point,sam_point)
#Make covariance matrix with expanded coordinates
#(확장된 좌표들의 covaraicne matrix를 만듭니다.)
total_chordal_dist = chordal_dist(total_point)
range=2*sin(MLE[2]/2)*6371
total_CovMat = covariance_func(total_chordal_dist, MLE[1], range, MLE[3], nugget)
#Divide covariance matrix into 4 submatrices
subMat_11=total_CovMat[1:nrow(rem_point),1:nrow(rem_point)]
subMat_12=total_CovMat[1:nrow(rem_point),(nrow(rem_point)+1):nrow(total_point)]
subMat_21=total_CovMat[(nrow(rem_point)+1):nrow(total_point),1:nrow(rem_point)]
subMat_22=total_CovMat[(nrow(rem_point)+1):nrow(total_point),(nrow(rem_point)+1):nrow(total_point)]
tryCatch({
inverse_subMat_11=solve(subMat_11)
sam_pred_vec=subMat_21%*%inverse_subMat_11%*%dev_matrix + mean
#After making kriging model, we have to check MSE to check 성능
MSE= mean((sam_pred_vec - test_data$tem)^2)
},error=function(e){
MSE =NA
})
MSE
MSE =NA
tryCatch({
inverse_subMat_11=solve(subMat_11)
sam_pred_vec=subMat_21%*%inverse_subMat_11%*%dev_matrix + mean
#After making kriging model, we have to check MSE to check 성능
MSE= mean((sam_pred_vec - test_data$tem)^2)
},error=function(e){
})
MSE
#Calculate negative log likelihood for optim
NegLogL_optim2=function(log_paras){
value= NA
tryCatch({
a=exp(log_paras[1])  #sill
b=exp(log_paras[2])  #spatial range
nu=exp(log_paras[3]) #smoothness parameter
x=chordal_matrix
covariance_matrix = covariance_func(x,a,b,nu,nugget)
det=as.double(determinant(covariance_matrix)$modulus)
inverse_matrix=solve(covariance_matrix)
value=(det+t(dev_matrix)%*%inverse_matrix%*%(dev_matrix))/2
},error=function(e){
})
if (is.na(value)){
cat('there is error in optim with',exp(log_paras),'\n')
value=0
}
return(value)
}
MLE_test2=function(train_data, test_data){
rem_point=as.matrix(train_data[,c("Long","Lat")])
sam_point=as.matrix(test_data[,c('Long',"Lat")])
train_dev=train_data$tem - mean(train_data$tem)
train.sp=SpatialPointsDataFrame(coords=as.matrix(train_data[,c('Long',"Lat")]),train_data,
proj4string=CRS("+proj=utm +zone=48 +datum=WGS84"))
chordal_matrix <<- theta_dist(rem_point)
#Find Initial value
v <- variogram((train_dev) ~ 1, locations = coordinates(rem_point), train.sp)
psill=max(v$gamma)
range= v$dist[length(v$dist)]
nugget <<- min(v$gamma)
smoothness_parameter= 1/2
parameters=c(psill, range, smoothness_parameter)
log_paras=log(abs(parameters)+0.001)
#Deviation matrix
mean <- mean(train_data$tem)
dev_matrix <<- as.matrix(train_data$tem - mean)
NLM=nlm(NegLogL_optim2,log_paras)
MLE=exp(NLM$estimate)
#Now, we are ready to predict temperature of unknown points
total_point=rbind(rem_point,sam_point)
#Make covariance matrix with expanded coordinates
#(확장된 좌표들의 covaraicne matrix를 만듭니다.)
total_chordal_dist = chordal_dist(total_point)
range=2*sin(MLE[2]/2)*6371
total_CovMat = covariance_func(total_chordal_dist, MLE[1], range, MLE[3], nugget)
#Divide covariance matrix into 4 submatrices
subMat_11=total_CovMat[1:nrow(rem_point),1:nrow(rem_point)]
subMat_12=total_CovMat[1:nrow(rem_point),(nrow(rem_point)+1):nrow(total_point)]
subMat_21=total_CovMat[(nrow(rem_point)+1):nrow(total_point),1:nrow(rem_point)]
subMat_22=total_CovMat[(nrow(rem_point)+1):nrow(total_point),(nrow(rem_point)+1):nrow(total_point)]
MSE =NA
tryCatch({
inverse_subMat_11=solve(subMat_11)
sam_pred_vec=subMat_21%*%inverse_subMat_11%*%dev_matrix + mean
#After making kriging model, we have to check MSE to check 성능
MSE= mean((sam_pred_vec - test_data$tem)^2)
},error=function(e){
})
#kriging(based on Lecture 03)
return(c(MLE[1], range, MLE[3], nugget, MSE))
}
for(file in folder[3:16]){
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
count=count+1
}
for(file in folder[4:16]){
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
count=count+1
}
count=which(file == folder)
count
folder_path <-"C:/Users/howsr/OneDrive/공간통계 학술제/데이터/Renewed Data/Renewed Data"
Spring_files <- list.files(path = folder_path, pattern = "Spring\\.xlsx$", full.names = TRUE)
Summer_files <- list.files(path = folder_path, pattern = "Summer\\.xlsx$", full.names = TRUE)
Fall_files <- list.files(path = folder_path, pattern = "Fall\\.xlsx$", full.names = TRUE)
Winter_files <- list.files(path = folder_path, pattern = "Winter\\.xlsx$", full.names = TRUE)
folder=c(Spring_files, Summer_files, Fall_files, Winter_files)
set.seed(1111)
Compare_models_result=matrix(NA,nrow=16,ncol=4)
models=c('kriging based on variogram','kriging based on MLE','IDW interpolation','Numeric interpolation')
colnames(Compare_models_result)=models
All_result=matrix(NA,nrow=192,ncol=4)
colnames(All_result)=models
for(file in folder[4:16]){
count=which(file == folder)
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
}
for(file in folder[4:16]){
count=which(file == folder)-1
df= read_excel(path=file)
data = df
df1 =data[1:4]
df2=data[6]
df3=cbind(df1,df2)
data=df3
colnames(data) <- c("Index","Point","Lat","Long","tem")
folds <- createFolds(data$tem, k = 10, list = TRUE)
for (k in 1:10) {
train_data <- data[-folds[[k]], ]
test_data <- data[folds[[k]], ]
All_result[(k+count*12),1]=mat_test(train_data,test_data)  #=vario_test(train_data,test_data)[4]
All_result[(k+count*12),2]=MLE_test2(train_data,test_data)[5]
All_result[(k+count*12),3]=idw_test(train_data,test_data)[2]
All_result[(k+count*12),4]=tps_test(train_data,test_data)
print(All_result[(1+count*12):(10+count*12),1:4])
all.data.frame=as.data.frame(All_result)
write.xlsx(all.data.frame, file="Compare models all result save.xlsx" ,overwite=TRUE)
}
All_result[count*12+11,]=colMeans(All_result[(1+count*12):(10+count*12),],na.rm = TRUE)
Compare_models_result[count+1,] = All_result[count*12+11,]
print(Compare_models_result)
mean.data.frame=as.data.frame(Compare_models_result)
write.xlsx(mean.data.frame, file="Compare models result save.xlsx",overwite=TRUE)
}
library(stats4)
library(readxl)       ## Load the Data
library(openxlsx)     ## (20230911) Save 예측 데이터
library(RColorBrewer) ## colorRampPalette
library(geodist)      ## Calculate geodesic distance
library(base)         ## besselK
library(dplyr)        ## For Using pipe operator %>% and making random sample
library(sp)           ## For Making Spatial Points Data Frame(SPDF)
library(maptools)     ## For Making grid Data
#maptools가 없어질 예정임. 우리가 사용하는 Sobj_SpatialGrid가 sp로 넘어갈지는 모르겠음,
#대신 SpatialGrid 패키지의 GridTopology라는 함수를 사용 할 생각도 해야할 듯
library(gstat)        ## Sample Variogram, IDW interpolation
library(caret)        ## (20230911) k-fold
All_result2=read_excel("CorVer Compare models all result.xlsx")
All_result=as.matrix(All_result2)
Compare_models_variance=matrix(NA,nrow=16,ncol=4)
Compare_models_mean=matrix(NA,nrow=16,ncol=4)
Compare_models_trimed_mean=matrix(NA,nrow=16,ncol=4)
for (count in 0:15){
Compare_models_mean[count+1,] =
apply(All_result[(1+count*12):(10+count*12),], 2, function(x) mean(x, na.rm = TRUE))
Compare_models_variance[count+1,] =
apply(All_result[(1+count*12):(10+count*12),], 2, function(x) var(x, na.rm = TRUE))
Compare_models_trimed_mean[count+1,] =
apply(All_result[(1+count*12):(10+count*12),], 2, function(x) mean(x, trim = 0.2, na.rm = TRUE))
}
Compare_models_variance
Compare_models_mean
Compare_models_trimed_mean
mean.df=as.data.frame(Compare_models_mean)
var.df=as.data.frame(Compare_models_variance)
trimmean.df=as.data.frame(Compare_models_trimed_mean)
compare_model_summary =matrix(NA, nrow=3, ncol=4)
compare_model_summary[1,]= apply(Compare_models_mean, 2, mean)
compare_model_summary[2,]= apply(Compare_models_trimed_mean, 2, mean)
compare_model_summary[3,]= apply(Compare_models_variance, 2, mean)
compare_model_summary
summary.df=as.data.frame(compare_model_summary)
write.xlsx(trimmean.df, file="Compare models trimmed mean for correction data.xlsx",overwite=FALSE)
write.xlsx(var.df, file="Compare models varience for correction data.xlsx",overwite=FALSE)
write.xlsx(mean.df, file="Compare models mean for correction data.xlsx",overwite=FALSE)
write.xlsx(summary.df, file="Compare models summary for correction data.xlsx",overwite=FALSE)
